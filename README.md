# â›° "Ã–lÃ§ekli EÄŸitim" Ãœnitesi ğŸ—»

Bu Ã¼nitede, WorkinTech'teki Veri Bilimi ekibi tarafÄ±ndan saÄŸlanan notebook'u nasÄ±l paketleyeceÄŸinizi ve tam veri seti Ã¼zerinde yerel olarak eÄŸitilebilmesi iÃ§in nasÄ±l Ã¶lÃ§eklendireceÄŸinizi Ã¶ÄŸreneceksiniz.

Bu Ã¼nite aÅŸaÄŸÄ±daki 5 challenge'dan oluÅŸur ve hepsi bu tek `README` dosyasÄ±nda gruplandÄ±rÄ±lmÄ±ÅŸtÄ±r.

Sadece kÄ±lavuzu takip edin ve ilerlemelerinizi izleyebilmemiz iÃ§in her ana bÃ¶lÃ¼mden sonra `git push` yapÄ±n!

# 1ï¸âƒ£ Yerel Kurulum

<details>
  <summary markdown='span'><strong>â“Instructions (expand me)</strong></summary>

Projenin baÅŸ ML MÃ¼hendisi olarak, ilk rolÃ¼nÃ¼z yerel bir Ã§alÄ±ÅŸma ortamÄ± (pyenv ile) ve yalnÄ±zca kod tabanÄ±nÄ±zÄ±n iskeletini iÃ§eren bir python paketi kurmaktÄ±r.

ğŸ’¡ Notebook'larÄ± paketlemek temel bir ML MÃ¼hendisi becerisidir. Bu ÅŸunlarÄ± saÄŸlar:
- diÄŸer kullanÄ±cÄ±larÄ±n kod Ã¼zerinde iÅŸbirliÄŸi yapabilmesi
- kodu yerel olarak veya uzak bir makinede klonlayabilmeniz, Ã¶rneÄŸin `taxifare` modelini daha gÃ¼Ã§lÃ¼ bir makinede eÄŸitmek iÃ§in
- kodu bir **API** olarak veya bir **web sitesi** aracÄ±lÄ±ÄŸÄ±yla sunmak Ã¼zere Ã¼retime koyabilmeniz
- kodu manuel olarak Ã§alÄ±ÅŸtÄ±rÄ±labilir veya otomasyon iÅŸ akÄ±ÅŸÄ±na entegre edilebilir hale getirebilmeniz

### 1.1) [ğŸ taxifare-env] adÄ±nda yeni bir pyenv oluÅŸturun

WorkinTech'in ML yÄ±ÄŸÄ±nÄ± Python 3.10.6 Ã¼zerinde Ã§alÄ±ÅŸÄ±yor, WorkinTech'e iyi hizmet veren kararlÄ± bir sÃ¼rÃ¼m: *"Bozuk deÄŸilse, tamir etme."* O halde aynÄ± Python sÃ¼rÃ¼mÃ¼nÃ¼ kullanalÄ±m.

ğŸ Python 3.10.6'yÄ± kurun

```bash
pyenv install 3.10.6
```

ğŸ Virtual ortamÄ± oluÅŸturun

```bash
cd ~/code/{{local_path_to("07-ML-Ops/01-Train-at-scale/01-Train-at-scale")}}
```

```bash
pyenv virtualenv 3.10.6 taxifare-env
pyenv local taxifare-env
pip install --upgrade pip
code .
```

Daha sonra, hem iÅŸletim sisteminizin Terminali hem de VS Code'un entegre Terminalinin `[ğŸ taxifare-env]` gÃ¶sterdiÄŸinden emin olun.
VS Code'da herhangi bir `.py` dosyasÄ± aÃ§Ä±n ve aÅŸaÄŸÄ±da gÃ¶sterildiÄŸi gibi saÄŸ alttaki pyenv bÃ¶lÃ¼mÃ¼ne tÄ±klayarak `taxifare-env`'in aktif olduÄŸunu kontrol edin:

<a href="/pyenv-setup.png" target="_blank">
    <img src='/pyenv-setup.png' width=400>
</a>

### 1.2) taxifare paket yapÄ±sÄ±na aÅŸina olun

â—ï¸Sizin iÃ§in hazÄ±rladÄ±ÄŸÄ±mÄ±z ÅŸablonun yapÄ±sÄ±nÄ± anlamak iÃ§in 10 dakika ayÄ±rÄ±n (detaya girmeyin); giriÅŸ noktasÄ± `taxifare.interface.main_local`'dir: hÄ±zlÄ±ca takip edin.

```bash
. # Challenge klasÃ¶r kÃ¶kÃ¼
â”œâ”€â”€ Makefile          # ğŸšª Komut "baÅŸlatÄ±cÄ±nÄ±z". YaygÄ±n olarak kullanÄ±n (eÄŸitim baÅŸlatma, testler, vb...)
â”œâ”€â”€ README.md         # Åu anda okuduÄŸunuz dosya!
â”œâ”€â”€ notebooks
â”‚   â””â”€â”€ datascientist_deliverable.ipynb   # DS ekibinden teslim edilecek dosya!
â”œâ”€â”€ requirements.txt   # Yerel ortamÄ±nÄ±za eklenecek tÃ¼m Ã¼Ã§Ã¼ncÃ¼ taraf paketleri listeler
â”œâ”€â”€ setup.py           # Paketiniz iÃ§in `pip install`'Ä± etkinleÅŸtirir
â”œâ”€â”€ taxifare           # Bu paketin kod mantÄ±ÄŸÄ±
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ interface
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ main_local.py  # ğŸšª TÃ¼m "rotalarÄ±" iÃ§eren ana Python giriÅŸ noktanÄ±z
â”‚   â””â”€â”€ ml_logic
â”‚   |    â”œâ”€â”€ __init__.py
â”‚   |    â”œâ”€â”€ data.py           # Veri kaydetme, yÃ¼kleme ve temizleme
â”‚   |    â”œâ”€â”€ encoders.py       # Ã–zel encoder yardÄ±mcÄ±larÄ±
â”‚   |    â”œâ”€â”€ model.py          # TensorFlow modeli
â”‚   |    â”œâ”€â”€ preprocessor.py   # Sklearn Ã¶n iÅŸleme ardÄ±ÅŸÄ±k dÃ¼zenleri
â”‚   |    â”œâ”€â”€ registry.py       # Modelleri kaydetme ve yÃ¼kleme
|   â”œâ”€â”€ utils.py    # # taxifare mantÄ±ÄŸÄ±na baÄŸÄ±mlÄ± olmayan yararlÄ± python fonksiyonlarÄ±
|   â”œâ”€â”€ params.py   # Global proje parametreleri
|
â”œâ”€â”€ tests  # `make test_...` kullanarak Ã§alÄ±ÅŸtÄ±rÄ±lacak testler
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ ...
â”œâ”€â”€ .gitignore
```

ğŸ Paketinizi bu yeni virtual ortama kurun

```bash
cd ~/code/{{local_path_to("07-ML-Ops/01-Train-at-scale/01-Train-at-scale")}}
pip install -e .
```

`pip list | grep taxifare` Ã§alÄ±ÅŸtÄ±rarak paketin kurulduÄŸundan emin olun; paketin mutlak yolunu yazdÄ±rmalÄ±dÄ±r.


### 1.3) Veri nerede?

**Ham veri Google Big Query'de**

WorkinTech'Ä±n mÃ¼hendislik ekibi, 2009'dan beri tÃ¼m taksi yolculuÄŸu geÃ§miÅŸini devasa bir Big Query tablosunda `data-analytics-469406.mlops.mlops_all` saklar.

**Google Cloud Platform eriÅŸimini kontrol edin**

ğŸ§ª Her ÅŸeyin yolunda olduÄŸunu kontrol edin
```bash
make test_gcp_setup
```

**BQ'yu iki kez sorgulamaktan kaÃ§Ä±nmak iÃ§in tÃ¼m ara verileri her zaman yerel olarak `~/.workintech/mlops/` iÃ§inde Ã¶nbellekte saklarÄ±z**

ğŸ’¾ `data` klasÃ¶rÃ¼mÃ¼zÃ¼ bu challenge klasÃ¶rÃ¼nÃ¼n *dÄ±ÅŸÄ±nda* saklayalÄ±m, bÃ¶ylece tÃ¼m ML Ops modÃ¼lÃ¼ boyunca diÄŸer tÃ¼m challenge'lar tarafÄ±ndan eriÅŸilebilir. Zaten `git` tarafÄ±ndan takip edilmesini istemiyoruz!

``` bash
# Create the data folder
mkdir -p ~/workintech/mlops/data/

# Create relevant subfolders
mkdir ~/workintech/mlops/data/raw
mkdir ~/workintech/mlops/data/processed
```

ğŸ’¡Buradayken, tÃ¼m challenge'lar tarafÄ±ndan paylaÅŸÄ±lacak `training_outputs` iÃ§in de bir depolama klasÃ¶rÃ¼ oluÅŸturalÄ±m

```bash
# Create the training_outputs folder
mkdir ~/workintech/mlops/training_outputs

# Create relevant subfolders
mkdir ~/workintech/mlops/training_outputs/metrics
mkdir ~/workintech/mlops/training_outputs/models
mkdir ~/workintech/mlops/training_outputs/params
```

ArtÄ±k gelecek challenge'lar iÃ§in verilerin, Veri Bilimi ekibinin notebook'larÄ± ve model Ã§Ä±ktÄ±larÄ± ile birlikte `~/workintech/mlops/` iÃ§inde depolandÄ±ÄŸÄ±nÄ± gÃ¶rebilirsiniz:

``` bash
tree -a ~/workintech/mlops/

# BUNU GÃ–RMELÄ°SÄ°NÄ°Z
â”œâ”€â”€ data          # Burada ÅŸunlarÄ± yapacaksÄ±nÄ±z:
â”‚   â”œâ”€â”€ processed # Ara, iÅŸlenmiÅŸ verileri saklama
â”‚   â””â”€â”€ raw       # Ham veri Ã¶rneklerini indirme
â””â”€â”€ training_outputs
    â”œâ”€â”€ metrics # EÄŸitilmiÅŸ model metriklerini saklama
    â”œâ”€â”€ models  # EÄŸitilmiÅŸ model aÄŸÄ±rlÄ±klarÄ±nÄ± saklama (bÃ¼yÃ¼k olabilir!)
    â””â”€â”€ params  # EÄŸitilmiÅŸ model hiperparametrelerini saklama
```

â˜ï¸ Ä°stediÄŸiniz zaman tÃ¼m dosyalarÄ± kaldÄ±rÄ±p bu boÅŸ klasÃ¶r yapÄ±sÄ±nÄ± korumak iÃ§in ÅŸunu kullanabilirsiniz

```bash
make reset_local_files
```

</details>

# 2ï¸âƒ£ Veri Bilimcisinin Ã‡alÄ±ÅŸmasÄ±nÄ± Anlama

<details>
  <summary markdown='span'><strong>â“Instructions (expand me)</strong></summary>

*â± SÃ¼re: buna 1 saat ayÄ±rÄ±n*

ğŸ–¥ï¸ `datascientist_deliverable.ipynb`'i VS Code ile aÃ§Ä±n (bu modÃ¼l iÃ§in Jupyter'i unutun) ve tÃ¼m hÃ¼creleri dikkatli bir ÅŸekilde Ã§alÄ±ÅŸtÄ±rÄ±n, onlarÄ± anlarken. Sizinle DS ekibi arasÄ±ndaki bu devir teslim, onlarla etkileÅŸime geÃ§mek (yani arkadaÅŸÄ±nÄ±z veya bir TA) iÃ§in mÃ¼kemmel zaman.

â—ï¸`taxifare-env`'i bir `ipykernel` venv olarak kullandÄ±ÄŸÄ±nÄ±zdan emin olun

<a href="/pyenv-notebook.png" target="_blank">
    <img src='/pyenv-notebook.png' width=400>
</a>

</details>


# 3ï¸âƒ£ Kodunuzu Paketleyin

<details>
  <summary markdown='span'><strong>â“Instructions (expand me)</strong></summary>

ğŸ¯ Hedefiniz aÅŸaÄŸÄ±da gÃ¶rÃ¼ldÃ¼ÄŸÃ¼ gibi `taxifare.interface.main_local` modÃ¼lÃ¼nÃ¼ Ã§alÄ±ÅŸtÄ±rabilmektir

```bash
# -> model
python -m taxifare.interface.main_local
```

ğŸ–¥ï¸ Bunu yapmak iÃ§in, lÃ¼tfen aÅŸaÄŸÄ±daki dosyalarda `# YOUR CODE HERE` ile iÅŸaretlenmiÅŸ eksik bÃ¶lÃ¼mleri kodlayÄ±n; Notebook'u oldukÃ§a yakÄ±ndan takip etmelidir!

```bash
â”œâ”€â”€ taxifare
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ interface
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ main_local.py   # ğŸ”µ ğŸšª Entry point: code both `preprocess_and_train()` and `pred()`
â”‚   â””â”€â”€ ml_logic
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ data.py          # ğŸ”µ your code here
â”‚       â”œâ”€â”€ encoders.py      # ğŸ”µ your code here
â”‚       â”œâ”€â”€ model.py         # ğŸ”µ your code here
â”‚       â”œâ”€â”€ preprocessor.py  # ğŸ”µ your code here
â”‚       â”œâ”€â”€ registry.py  # âœ… `save_model` and `load_model` are already coded for you
|   â”œâ”€â”€ params.py # ğŸ”µ You need to fill your GCP_PROJECT
â”‚   â”œâ”€â”€ utils.py
```

**ğŸ§ª Kodunuzu test edin**

Åu anki taxifare-env ortamÄ±nda paketin doÄŸru kurulduÄŸundan emin olun, eÄŸer deÄŸilse

```bash
pip list | grep taxifare
```

Daha sonra, paketinizin `python -m taxifare.interface.main_local` ile dÃ¼zgÃ¼n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun.
- Ã‡alÄ±ÅŸana kadar hata ayÄ±klama yapÄ±n!
- AÅŸaÄŸÄ±daki veri seti boyutlarÄ±nÄ± kullanÄ±n

```python
# taxifare/ml_logic/params.py
DATA_SIZE = '1k'   # To iterate faster in debug mode ğŸ
DATA_SIZE = '200k' # Should work at least once
# DATA_SIZE = 'all' ğŸš¨ DON'T TRY YET, it's too big and will cost money!
```

Daha sonra, yalnÄ±zca `make test_preprocess_and_train` ile testleri geÃ§meye Ã§alÄ±ÅŸÄ±n!

âœ… TÃ¼m testler yeÅŸil olduÄŸunda, sonuÃ§larÄ±nÄ±zÄ± kitt'te `make test_kitt` ile takip edin

</details>

# 4ï¸âƒ£ Ã–lÃ§eklenebilirliÄŸi AraÅŸtÄ±rÄ±n

<details>
  <summary markdown='span'><strong>â“Instructions (expand me)</strong></summary>

*â± SÃ¼re: buna en fazla 20 dakika ayÄ±rÄ±n*

ArtÄ±k paketi kÃ¼Ã§Ã¼k bir veri seti iÃ§in Ã§alÄ±ÅŸtÄ±rmayÄ± baÅŸardÄ±ÄŸÄ±nÄ±za gÃ¶re, gerÃ§ek veri setiyle nasÄ±l baÅŸ a Ã§Ä±kacaÄŸÄ±nÄ± gÃ¶rme zamanÄ±!

ğŸ‘‰ Ciddiye almaya baÅŸlamak iÃ§in `ml_logic.params.DATA_SIZE`'i `all` olarak deÄŸiÅŸtirin!

ğŸ•µï¸ Kodunuzun hangi bÃ¶lÃ¼mÃ¼nÃ¼n **en Ã§ok zaman** aldÄ±ÄŸÄ±nÄ± ve **en Ã§ok bellek** kullandÄ±ÄŸÄ±nÄ± `taxifare.utils.simple_time_and_memory_tracker`'Ä± seÃ§tiÄŸiniz metodlara dekore etmek iÃ§in kullanarak araÅŸtÄ±rÄ±n.

```python
# taxifare.ml_logic.data.py
from taxifare.utils import simple_time_and_memory_tracker

@simple_time_and_memory_tracker
def clean_data() -> pd.DataFrame:
    ...
```

ğŸ•µï¸ ArkadaÅŸÄ±nÄ±zla aÅŸaÄŸÄ±daki sorularÄ± cevaplamaya Ã§alÄ±ÅŸÄ±n:
- Kodunuzun hangi bÃ¶lÃ¼mÃ¼ temel dar boÄŸazlarÄ± barÄ±ndÄ±rÄ±r?
- Hangi tÃ¼r dar boÄŸazlar en endiÅŸe verici? (zaman? bellek?)
- Size 50M satÄ±r verseydi Ã¶lÃ§ekleneceÄŸini dÃ¼ÅŸÃ¼nÃ¼yor musunuz? 500M? Bu arada, [gerÃ§ek NYC veri seti](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) daha da bÃ¼yÃ¼k ve yaklaÅŸÄ±k 156GB aÄŸÄ±rlÄ±ÄŸÄ±nda!
- Potansiyel Ã§Ã¶zÃ¼mler hakkÄ±nda dÃ¼ÅŸÃ¼nebiliyor musunuz? Fikirlerinizi yazÄ±n, ama henÃ¼z uygulama yapmayÄ±n!
</details>


# 5ï¸âƒ£ ArttÄ±rÄ±mlÄ± Ä°ÅŸleme

<details>
  <summary markdown='span'><strong>â“Talimatlar (beni geniÅŸlet)</strong></summary>

ğŸ¯ Hedefiniz kod tabanÄ±nÄ±zÄ± sÄ±nÄ±rsÄ±z miktarda satÄ±r Ã¼zerinde model eÄŸitebilecek ÅŸekilde geliÅŸtirmektir, **RAM sÄ±nÄ±rlarÄ±na ulaÅŸmadan**, tek bir bilgisayarda.

## 5.1) TartÄ±ÅŸma

**Ne Ã¶ÄŸrendik?**

Bellek ve zaman kÄ±sÄ±tlarÄ±mÄ±z var:
- `(55M, 8)` boyutlu ham veri bellek iÃ§inde DataFrame olarak yÃ¼klendirilir ve yaklaÅŸÄ±k 10GB RAM kullanÄ±r, bu da Ã§oÄŸu bilgisayar iÃ§in Ã§ok fazladÄ±r.
- `(55M, 65)` boyutlu Ã¶n iÅŸlemli DataFrame daha da bÃ¼yÃ¼ktÃ¼r.
- `ml_logic.encoders.compute_geohash` metodu iÅŸleme Ã§ok uzun zaman alÄ±r ğŸ¤¯

Bir Ã§Ã¶zÃ¼m yeterli RAM'i olan bir *bulut Sanal Makinesi (VM)* iÃ§in para Ã¶deyerek onu orada iÅŸlemektir (bÃ¶yle bir problemle baÅŸ etmenin genellikle en basit yolu budur).

**Ã–nerilen Ã§Ã¶zÃ¼m: arttÄ±rÄ±mlÄ± Ã¶n iÅŸleme ğŸ”ª parÃ§a parÃ§a ğŸ”ª**

<img src="/process_by_chunk.png" width=500>

ğŸ’¡ Ã–n iÅŸlemcimiz *durumsuz* olduÄŸundan, kolayca ÅŸunlarÄ± yapabiliriz:
- Herhangi bir _sÃ¼tun bazlÄ± istatistik_ hesaplamasÄ±ndan kaÃ§Ä±nÄ±p yalnÄ±zca _satÄ±r bazÄ±nda Ã¶n iÅŸleme_ gerÃ§ekleÅŸtirin
- _Ã¶n iÅŸleme_'yi _eÄŸitim_'den ayÄ±rÄ±n ve herhangi bir ara sonuÃ§ diskte saklayÄ±n!

ğŸ™ Bu nedenle, sÄ±nÄ±rlÄ± boyutta parÃ§alar (Ã–r. 100.000 satÄ±r) ile Ã¶n iÅŸlemeyi *parÃ§a parÃ§a* yapalÄ±m, her parÃ§a belleÄŸe gÃ¼zelÃ§e sÄ±ÄŸacak:

1. `data_processed_chunk_01`'i sabit diskte saklarayÄ±z.
2. Sonra `data_processed_chunk_02`'yi ilkine ekleriz.
3. vb...
4. `~/workintech/mlops/data/processed/processed_all.csv` adresinde devasa bir CSV depolanana kadar

5. BÃ¶lÃ¼m 6ï¸âƒ£'te, modelimizi de parÃ§a parÃ§a `train()` ederiz, her parÃ§ada yÃ¼kleme ve eÄŸitimi tekrarlÄ± olarak yaparak (sonraki bÃ¶lÃ¼mde daha fazlasÄ±)

## 5.2) SÄ±ranÄ±z: `def preprocess()` kodlayÄ±n

ğŸ‘¶ **Ã–nce, hata ayÄ±klama amaÃ§larÄ± iÃ§in daha kÃ¼Ã§Ã¼k veri seti boyutlarÄ±nÄ± geri getirelim**

```python
# params.py
DATA_SIZE = '1k'
CHUNK_SIZE = 200
```

**Daha sonra, `ml_logic.interface.main_local` modÃ¼lÃ¼nÃ¼zdeki `def preprocess()` ile aÅŸaÄŸÄ±da verilen yeni rotayÄ± kodlayÄ±n; baÅŸlamak iÃ§in aÅŸaÄŸÄ±daki kodu kopyalayÄ±p yapÄ±ÅŸtÄ±rÄ±n**

[//]: # (  ğŸš¨ Code below is NOT the single source of truth. Original is in data-solutions repo ğŸš¨ )

<br>

<details>
  <summary markdown='span'>ğŸ‘‡ Kopyalanacak kod ğŸ‘‡</summary>

```python
def preprocess(min_date: str = '2009-01-01', max_date: str = '2015-01-01') -> None:
    """
    Query and preprocess the raw dataset iteratively (by chunks).
    Then store the newly processed (and raw) data on local hard-drive for later re-use.

    - If raw data already exists on local disk:
        - use `pd.read_csv(..., chunksize=CHUNK_SIZE)`

    - If raw data does not yet exists:
        - use `bigquery.Client().query().result().to_dataframe_iterable()`

    """
    print(Fore.MAGENTA + "\n â­ï¸ Use case: preprocess by batch" + Style.RESET_ALL)

    from taxifare.ml_logic.data import clean_data
    from taxifare.ml_logic.preprocessor import preprocess_features

    min_date = parse(min_date).strftime('%Y-%m-%d') # e.g '2009-01-01'
    max_date = parse(max_date).strftime('%Y-%m-%d') # e.g '2009-01-01'

    query = f"""
        SELECT {",".join(COLUMN_NAMES_RAW)}
        FROM `{GCP_PROJECT_WORKINTECH}`.{BQ_DATASET}.raw_{DATA_SIZE}
        WHERE pickup_datetime BETWEEN '{min_date}' AND '{max_date}'
        ORDER BY pickup_datetime
        """
    # Retrieve `query` data as dataframe iterable
    data_query_cache_path = Path(LOCAL_DATA_PATH).joinpath("raw", f"query_{min_date}_{max_date}_{DATA_SIZE}.csv")
    data_processed_path = Path(LOCAL_DATA_PATH).joinpath("processed", f"processed_{min_date}_{max_date}_{DATA_SIZE}.csv")

    data_query_cache_exists = data_query_cache_path.is_file()
    if data_query_cache_exists:
        print("Get a dataframe iterable from local CSV...")
        chunks = None
        # YOUR CODE HERE

    else:
        print("Get a dataframe iterable from Querying Big Query server...")
        chunks = None
        # ğŸ¯ Hints: `bigquery.Client(...).query(...).result(page_size=...).to_dataframe_iterable()`
        # YOUR CODE HERE

    for chunk_id, chunk in enumerate(chunks):
        print(f"processing chunk {chunk_id}...")

        # Clean chunk
        # YOUR CODE HERE

        # Create chunk_processed
        # ğŸ¯ Hints: Create (`X_chunk`, `y_chunk`), process only `X_processed_chunk`, then concatenate (X_processed_chunk, y_chunk)
        # YOUR CODE HERE

        # Save and append the processed chunk to a local CSV at "data_processed_path"
        # ğŸ¯ Hints: df.to_csv(mode=...)
        # ğŸ¯ Hints: We want a CSV without index nor headers (they'd be meaningless)
        # YOUR CODE HERE

        # Save and append the raw chunk if not `data_query_cache_exists`
        # YOUR CODE HERE
    print(f"âœ… data query saved as {data_query_cache_path}")
    print("âœ… preprocess() done")


```

</details>

<br>

**â“AÅŸaÄŸÄ±daki Ã¶n iÅŸlemli veri setlerini oluÅŸturup saklamaya Ã§alÄ±ÅŸÄ±n**

- `preprocess()` Ã§alÄ±ÅŸtÄ±rarak `data/processed/train_processed_1k.csv`

<br>

**ğŸ§ª Kodunuzu test edin**

Kodunuzu `make test_preprocess_by_chunk` ile test edin.

âœ… TÃ¼m testler yeÅŸil olduÄŸunda, sonuÃ§larÄ±nÄ±zÄ± kitt'te `make test_kitt` ile takip edin

<br>

**â“Son olarak, gerÃ§ek Ã¶n iÅŸlemli veri setlerini oluÅŸturup saklayÄ±n**

Kullanarak:
```python
# params.py
DATA_SIZE = 'all'
CHUNK_SIZE = 100000
```

ğŸ‰ BirkaÃ§ saatlik hesaplama ile 55 Milyon satÄ±rÄ± da kolayca iÅŸleyebilirdik, ama bugÃ¼n yapmayaalÄ±m ğŸ˜…

</details>

# 6ï¸âƒ£ ArttÄ±rÄ±mlÄ± Ã–ÄŸrenme

<details>
  <summary markdown='span'><strong>â“Talimatlar (beni geniÅŸlet)</strong></summary>

<br>

ğŸ¯ Hedef: modelimizi tam `.../processed/processed_all.csv` Ã¼zerinde eÄŸitmek

## 6.1) TartÄ±ÅŸma

Teorik olarak, `(xxMilyonlar, 65)` boyutundaki bÃ¶yle bÃ¼yÃ¼k bir veri setini aynÄ± anda RAM'e yÃ¼kleyemeyiz, ama parÃ§alar halinde yÃ¼kleyebiliriz.

**Bir modeli parÃ§alar halinde nasÄ±l eÄŸitiriz?**

Buna **arttÄ±rÄ±mlÄ± Ã¶ÄŸrenme** veya **partial_fit** denir
- Rastgele aÄŸÄ±rlÄ±klarla bir model baÅŸlatÄ±rÄ±z ${\theta_0}$
- Ä°lk `data_processed_chunk`'i belleÄŸe yÃ¼kleriz (diyelim ki 100.000 satÄ±r)
- Modelimizi ilk parÃ§a Ã¼zerinde eÄŸitiriz ve aÄŸÄ±rlÄ±klarÄ±nÄ± buna gÃ¶re gÃ¼ncelleriz ${\theta_0} \rightarrow {\theta_1}$
- Ä°kinci `data_processed_chunk`'i belleÄŸe yÃ¼kleriz
- Modelimizi ikinci parÃ§a Ã¼zerinde *yeniden eÄŸitiriz*, bu sefer Ã¶nceden hesaplanmÄ±ÅŸ aÄŸÄ±rlÄ±klarÄ± gÃ¼ncelleriz ${\theta_1} \rightarrow {\theta_2}$!
- Veri setinin sonuna kadar tekrarlarÄ±z

â—ï¸TÃ¼m Makine Ã–ÄŸrenmesi modelleri arttÄ±rÄ±mlÄ± Ã¶ÄŸrenmeyi desteklemez; yalnÄ±zca Gradyan Ä°niÅŸi gibi *iteratif gÃ¼ncelleme metotlarÄ±*na dayalÄ± *parametrik* modeller $f_{\theta}$ bunu destekler
- **scikit-learn**'de, `model.partial_fit()` yalnÄ±zca SGDRegressor/Classifier ve birkaÃ§ diÄŸeri iÃ§in mevcuttur ([bunu dikkatli oku ğŸ“š](https://scikit-learn.org/0.15/modules/scaling_strategies.html#incremental-learning)).
- **TensorFlow** ve diÄŸer Derin Ã–ÄŸrenme Ã§erÃ§evelerinde, eÄŸitim her zaman iteratiftir ve arttÄ±rÄ±mlÄ± Ã¶ÄŸrenme varsayÄ±lan davranÄ±ÅŸtÄ±r! YalnÄ±zca iki parÃ§a arasÄ±nda `model.initialize()` Ã§aÄŸÄ±rmaktan kaÃ§Ä±nmanÄ±z gerekir!

â—ï¸Derin Ã–ÄŸrenmedeki `chunk_size`'Ä± `batch_size` ile karÄ±ÅŸtÄ±rmayÄ±n

ğŸ‘‰ Her (bÃ¼yÃ¼k) parÃ§a iÃ§in, modeliniz verileri birkaÃ§ epoch boyunca birden Ã§ok (kÃ¼Ã§Ã¼k) batch'te okuyacaktÄ±r

<img src='/train_by_chunk.png'>

ğŸ‘ **Avantajlar:** bu evrensel yaklaÅŸÄ±m Ã§erÃ§eve-baÄŸÄ±msÄ±zdÄ±r; `scikit-learn`, XGBoost, TensorFlow vb. ile kullanabilirsiniz.

ğŸ‘ **Dezavantajlar:** model *en son* parÃ§aya *ilk* olanlardan daha iyi uyacak ÅŸekilde Ã¶nyargÄ±lÄ± olacaktÄ±r. Bizim durumumuzda, eÄŸitim veri setimiz karÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ olduÄŸu iÃ§in bu bir sorun deÄŸil, ancak modeli Ã¼retimde olduktan sonra yeni verilerle kÄ±smi uydurmak yaparken bunu akÄ±lda tutmak Ã¶nemlidir.

<br>

<details>
  <summary markdown='span'><strong>ğŸ¤” TensorFlow ile gerÃ§ekten parÃ§alara ihtiyacÄ±mÄ±z var mÄ±?</strong></summary>

Elbette, TensorFlow veri setleri sayesinde, aÅŸaÄŸÄ±da gÃ¶rÃ¼ldÃ¼ÄŸÃ¼ gibi batch-by-batch veri seti yÃ¼klemesini kullanabileceÄŸiniz iÃ§in her zaman "parÃ§alara" ihtiyacÄ±nÄ±z olmayacaktÄ±r

```python
import tensorflow as tf

ds = tf.data.experimental.make_csv_dataset(data_processed_all.csv, batch_size=256)
model.fit(ds)
```

Bunu Recap'te gÃ¶receÄŸiz. Yine de, bu challenge'da size herhangi bir Ã§erÃ§eveye uygulanan evrensel parÃ§alarda arttÄ±rÄ±mlÄ± uyum metotunu Ã¶ÄŸretmek istiyoruz ve bu model Ã¼retime konulduktan sonra modelinizi yeni verilerle *kÄ±smen yeniden eÄŸitmek* iÃ§in yararlÄ± olacaktÄ±r.
</details>

<br>

## 6.2) SÄ±ranÄ±z - `def train()` kodlayÄ±n

**`ml_logic.interface.main_local` modÃ¼lÃ¼nÃ¼zdeki `def train()` ile aÅŸaÄŸÄ±da verilen yeni rotayÄ± kodlamaya Ã§alÄ±ÅŸÄ±n; baÅŸlamak iÃ§in aÅŸaÄŸÄ±daki kodu kopyalayÄ±p yapÄ±ÅŸtÄ±rÄ±n**

Yine, Ã§ok kÃ¼Ã§Ã¼k bir veri seti boyutu ile baÅŸlayÄ±n, daha sonra modelinizi 500k satÄ±r Ã¼zerinde eÄŸitin.

[//]: # (  ğŸš¨ Code below is not the single source of truth ğŸš¨ )

<details>
  <summary markdown='span'><strong>ğŸ‘‡ Kopyalanacak kod ğŸ‘‡</strong></summary>

```python
def train(min_date:str = '2009-01-01', max_date:str = '2015-01-01') -> None:
    """
    Incremental train on the (already preprocessed) dataset locally stored.
    - Loading data chunk-by-chunk
    - Updating the weight of the model for each chunk
    - Saving validation metrics at each chunks, and final model weights on local disk
    """

    print(Fore.MAGENTA + "\n â­ï¸ Use case:train by batch" + Style.RESET_ALL)
    from taxifare.ml_logic.registry import save_model, save_results
    from taxifare.ml_logic.model import (compile_model, initialize_model, train_model)

    data_processed_path = Path(LOCAL_DATA_PATH).joinpath("processed", f"processed_{min_date}_{max_date}_{DATA_SIZE}.csv")
    model = None
    metrics_val_list = []  # store each val_mae of each chunk

    # Iterate in chunks and partial fit on each chunk
    chunks = pd.read_csv(data_processed_path,
                         chunksize=CHUNK_SIZE,
                         header=None,
                         dtype=DTYPES_PROCESSED)

    for chunk_id, chunk in enumerate(chunks):
        print(f"training on preprocessed chunk nÂ°{chunk_id}")
        # You can adjust training params for each chunk if you want!
        learning_rate = 0.0005
        batch_size = 256
        patience=2
        split_ratio = 0.1 # Higher train/val split ratio when chunks are small! Feel free to adjust.

        # Create (X_train_chunk, y_train_chunk, X_val_chunk, y_val_chunk)
        train_length = int(len(chunk)*(1-split_ratio))
        chunk_train = chunk.iloc[:train_length, :].sample(frac=1).to_numpy()
        chunk_val = chunk.iloc[train_length:, :].sample(frac=1).to_numpy()

        X_train_chunk = chunk_train[:, :-1]
        y_train_chunk = chunk_train[:, -1]
        X_val_chunk = chunk_val[:, :-1]
        y_val_chunk = chunk_val[:, -1]

        # Train a model *incrementally*, and store the val MAE of each chunk in `metrics_val_list`
        # YOUR CODE HERE

    # Return the last value of the validation MAE
    val_mae = metrics_val_list[-1]

    # Save model and training params
    params = dict(
        learning_rate=learning_rate,
        batch_size=batch_size,
        patience=patience,
        incremental=True,
        chunk_size=CHUNK_SIZE
    )

    print(f"âœ… Trained with MAE: {round(val_mae, 2)}")

     # Save results & model
    save_results(params=params, metrics=dict(mae=val_mae))
    save_model(model=model)

    print("âœ… train() done")

def pred(X_pred: pd.DataFrame = None) -> np.ndarray:

    print(Fore.MAGENTA + "\n â­ï¸ Use case: pred" + Style.RESET_ALL)

    from taxifare.ml_logic.registry import load_model
    from taxifare.ml_logic.preprocessor import preprocess_features

    if X_pred is None:
       X_pred = pd.DataFrame(dict(
           pickup_datetime=[pd.Timestamp("2013-07-06 17:18:00", tz='UTC')],
           pickup_longitude=[-73.950655],
           pickup_latitude=[40.783282],
           dropoff_longitude=[-73.984365],
           dropoff_latitude=[40.769802],
           passenger_count=[1],
       ))

    model = load_model()
    X_processed = preprocess_features(X_pred)
    y_pred = model.predict(X_processed)

    print(f"âœ… pred() done")
    return y_pred

```

</details>

**ğŸ§ª Kodunuzu test edin**

`make test_train_by_chunk` ile kontrol edin

ğŸ ğŸ ğŸ ğŸ Tebrikler! ğŸ ğŸ ğŸ ğŸ


</details>
