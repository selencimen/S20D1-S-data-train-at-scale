{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Veri Ke≈üfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import PIL\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
    "\n",
    "from sklearn import set_config; set_config(display='diagram')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Veri Y√ºkleme"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**T√ºm veri**\n",
    "\n",
    "WorkinTech'ƒ±n m√ºhendislik ekibi 2009'dan beri t√ºm taksi yolculuƒüu ge√ßmi≈üini devasa bir Big Query tablosunda `mlops.mlops_all` saklar. \n",
    "\n",
    "**Eƒüitim a≈üamasƒ± verisi**\n",
    "\n",
    "- Eƒüitim a≈üamasƒ± `2015-01-01` tarihinde ger√ßekle≈üir. Modelimizi eƒüitmek i√ßin bu tarihten sonraki verilere eri≈üemeyiz\n",
    "- Veri bilimcinin notebook'u burada **200k rastgele √∂rneklenmi≈ü alt k√ºme** √ºzerinde bir ML modeli eƒüitir (b√∂ylece her ≈üey Veri Bilimcinin laptopunda RAM belleƒüine sƒ±ƒüar)\n",
    "- WorkinTech'in veri m√ºhendisliƒüi ekibi bu `mlops_all` tablosundan `raw_200k` adlƒ± bir `materialized view` olu≈üturmu≈ü ve DataScientist'e eƒüitimi i√ßin okuma eri≈üimi vermi≈ütir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DATA_PATH = Path('~').joinpath(\".workintech\", \"mlops\", \"data\").expanduser()\n",
    "GCP_PROJECT = \"seraphic-alloy-473611-c7\"\n",
    "BQ_DATASET = \"mlops\"\n",
    "DATA_SIZE = \"all\"  # mlops_all is your BigQuery table\n",
    "MIN_DATE = '2009-01-01'\n",
    "MAX_DATE = '2015-01-01'\n",
    "COLUMN_NAMES_RAW = ('fare_amount',\t'pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A≈üaƒüƒ±da `GCP_PROJECT`'inizi doldurun üëá**, Daha sonra ge√ßmi≈ü verileri (2015 √∂ncesi) sorgulayalƒ±m, tarihe g√∂re sƒ±ralanmƒ±≈ü (b√∂ylece eƒüitim/test ayrƒ±mƒ±nƒ± kronolojik olarak daha kolay yapabiliriz)!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT fare_amount,pickup_datetime,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count\n",
      "    FROM `data-analytics-469406`.mlops.mlops_all\n",
      "    WHERE pickup_datetime BETWEEN '2009-01-01' AND '2015-01-01'\n",
      "    ORDER BY pickup_datetime\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "    SELECT {\",\".join(COLUMN_NAMES_RAW)}\n",
    "    FROM `{GCP_PROJECT}`.{BQ_DATASET}.{BQ_DATASET}_{DATA_SIZE}\n",
    "    WHERE pickup_datetime BETWEEN '{MIN_DATE}' AND '{MAX_DATE}'\n",
    "    ORDER BY pickup_datetime\n",
    "    \"\"\"\n",
    "print(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BigQuery √ßaƒürƒ±sƒ±nƒ± yalnƒ±zca dosya yerel olarak mevcut deƒüilse yap.  \n",
    "Aksi halde, her notebook √ßalƒ±≈ütƒ±rmasƒ±nda BigQuery'yi √ßaƒüƒ±rmaktan ka√ßƒ±nmak i√ßin yerel olarak CSV olarak sakla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying Big Query server...\n"
     ]
    },
    {
     "ename": "Forbidden",
     "evalue": "403 POST https://bigquery.googleapis.com/bigquery/v2/projects/data-analytics-469406/jobs?prettyPrint=false: BigQuery API has not been used in project seraphic-alloy-473611-c7 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/bigquery.googleapis.com/overview?project=seraphic-alloy-473611-c7 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry.\n\nLocation: None\nJob ID: a6100811-5e17-44e1-b37c-fc4bae875dca\n [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'SERVICE_DISABLED', 'domain': 'googleapis.com', 'metadata': {'serviceTitle': 'BigQuery API', 'activationUrl': 'https://console.developers.google.com/apis/api/bigquery.googleapis.com/overview?project=seraphic-alloy-473611-c7', 'containerInfo': 'seraphic-alloy-473611-c7', 'consumer': 'projects/seraphic-alloy-473611-c7', 'service': 'bigquery.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'BigQuery API has not been used in project seraphic-alloy-473611-c7 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/bigquery.googleapis.com/overview?project=seraphic-alloy-473611-c7 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry.'}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Google developers console API activation', 'url': 'https://console.developers.google.com/apis/api/bigquery.googleapis.com/overview?project=seraphic-alloy-473611-c7'}]}]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m bigquery\n\u001b[1;32m     11\u001b[0m client \u001b[38;5;241m=\u001b[39m bigquery\u001b[38;5;241m.\u001b[39mClient(project\u001b[38;5;241m=\u001b[39mGCP_PROJECT)\n\u001b[0;32m---> 12\u001b[0m query_job \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m result \u001b[38;5;241m=\u001b[39m query_job\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m     14\u001b[0m df \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mto_dataframe()\n",
      "File \u001b[0;32m~/.pyenv/versions/taxifare-env/lib/python3.10/site-packages/google/cloud/bigquery/client.py:3404\u001b[0m, in \u001b[0;36mClient.query\u001b[0;34m(self, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry, api_method)\u001b[0m\n\u001b[1;32m   3393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _job_helpers\u001b[38;5;241m.\u001b[39mquery_jobs_query(\n\u001b[1;32m   3394\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3395\u001b[0m         query,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3401\u001b[0m         job_retry,\n\u001b[1;32m   3402\u001b[0m     )\n\u001b[1;32m   3403\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m api_method \u001b[38;5;241m==\u001b[39m enums\u001b[38;5;241m.\u001b[39mQueryApiMethod\u001b[38;5;241m.\u001b[39mINSERT:\n\u001b[0;32m-> 3404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_job_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_jobs_insert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3405\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjob_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjob_id_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjob_retry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unexpected value for api_method: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(api_method)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/taxifare-env/lib/python3.10/site-packages/google/cloud/bigquery/_job_helpers.py:158\u001b[0m, in \u001b[0;36mquery_jobs_insert\u001b[0;34m(client, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m query_job\n\u001b[0;32m--> 158\u001b[0m future \u001b[38;5;241m=\u001b[39m \u001b[43mdo_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# The future might be in a failed state now, but if it's\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# unrecoverable, we'll find out when we ask for it's result, at which\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# point, we may retry.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m job_id_given:\n",
      "File \u001b[0;32m~/.pyenv/versions/taxifare-env/lib/python3.10/site-packages/google/cloud/bigquery/_job_helpers.py:135\u001b[0m, in \u001b[0;36mquery_jobs_insert.<locals>.do_query\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m query_job \u001b[38;5;241m=\u001b[39m job\u001b[38;5;241m.\u001b[39mQueryJob(job_ref, query, client\u001b[38;5;241m=\u001b[39mclient, job_config\u001b[38;5;241m=\u001b[39mjob_config)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[43mquery_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core_exceptions\u001b[38;5;241m.\u001b[39mConflict \u001b[38;5;28;01mas\u001b[39;00m create_exc:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# The thought is if someone is providing their own job IDs and they get\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# their job ID generation wrong, this could end up returning results for\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# the wrong query. We thus only try to recover if job ID was not given.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m job_id_given:\n",
      "File \u001b[0;32m~/.pyenv/versions/taxifare-env/lib/python3.10/site-packages/google/cloud/bigquery/job/query.py:1379\u001b[0m, in \u001b[0;36mQueryJob._begin\u001b[0;34m(self, client, retry, timeout)\u001b[0m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"API call:  begin the job via a POST request\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m \n\u001b[1;32m   1361\u001b[0m \u001b[38;5;124;03mSee\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;124;03m    ValueError: If the job has already begun.\u001b[39;00m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1379\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mQueryJob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mGoogleAPICallError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1381\u001b[0m     exc\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m=\u001b[39m _EXCEPTION_FOOTER_TEMPLATE\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1382\u001b[0m         message\u001b[38;5;241m=\u001b[39mexc\u001b[38;5;241m.\u001b[39mmessage, location\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocation, job_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_id\n\u001b[1;32m   1383\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/taxifare-env/lib/python3.10/site-packages/google/cloud/bigquery/job/base.py:740\u001b[0m, in \u001b[0;36m_AsyncJob._begin\u001b[0;34m(self, client, retry, timeout)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m# jobs.insert is idempotent because we ensure that every new\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# job has an ID.\u001b[39;00m\n\u001b[1;32m    739\u001b[0m span_attributes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m: path}\n\u001b[0;32m--> 740\u001b[0m api_response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspan_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBigQuery.job.begin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspan_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspan_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_api_repr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_properties(api_response)\n",
      "File \u001b[0;32m~/.pyenv/versions/taxifare-env/lib/python3.10/site-packages/google/cloud/bigquery/client.py:827\u001b[0m, in \u001b[0;36mClient._call_api\u001b[0;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m create_span(\n\u001b[1;32m    825\u001b[0m         name\u001b[38;5;241m=\u001b[39mspan_name, attributes\u001b[38;5;241m=\u001b[39mspan_attributes, client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, job_ref\u001b[38;5;241m=\u001b[39mjob_ref\n\u001b[1;32m    826\u001b[0m     ):\n\u001b[0;32m--> 827\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call()\n",
      "File \u001b[0;32m~/.pyenv/versions/taxifare-env/lib/python3.10/site-packages/google/api_core/retry.py:283\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    280\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    282\u001b[0m )\n\u001b[0;32m--> 283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_deadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/taxifare-env/lib/python3.10/site-packages/google/api_core/retry.py:190\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, deadline, on_error)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/.pyenv/versions/taxifare-env/lib/python3.10/site-packages/google/cloud/_http/__init__.py:494\u001b[0m, in \u001b[0;36mJSONConnection.api_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    482\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    483\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    484\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m     extra_api_info\u001b[38;5;241m=\u001b[39mextra_api_info,\n\u001b[1;32m    491\u001b[0m )\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_http_response(response)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expect_json \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[0;31mForbidden\u001b[0m: 403 POST https://bigquery.googleapis.com/bigquery/v2/projects/data-analytics-469406/jobs?prettyPrint=false: BigQuery API has not been used in project seraphic-alloy-473611-c7 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/bigquery.googleapis.com/overview?project=seraphic-alloy-473611-c7 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry.\n\nLocation: None\nJob ID: a6100811-5e17-44e1-b37c-fc4bae875dca\n [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'SERVICE_DISABLED', 'domain': 'googleapis.com', 'metadata': {'serviceTitle': 'BigQuery API', 'activationUrl': 'https://console.developers.google.com/apis/api/bigquery.googleapis.com/overview?project=seraphic-alloy-473611-c7', 'containerInfo': 'seraphic-alloy-473611-c7', 'consumer': 'projects/seraphic-alloy-473611-c7', 'service': 'bigquery.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'BigQuery API has not been used in project seraphic-alloy-473611-c7 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/bigquery.googleapis.com/overview?project=seraphic-alloy-473611-c7 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry.'}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Google developers console API activation', 'url': 'https://console.developers.google.com/apis/api/bigquery.googleapis.com/overview?project=seraphic-alloy-473611-c7'}]}]"
     ]
    }
   ],
   "source": [
    "data_query_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"raw\", f\"query_{MIN_DATE}_{MAX_DATE}_{DATA_SIZE}.csv\")\n",
    "\n",
    "if data_query_cache_path.is_file():\n",
    "    print(\"load local file...\")\n",
    "    df = pd.read_csv(data_query_cache_path, parse_dates=['pickup_datetime'])\n",
    "\n",
    "else:\n",
    "    print(\"Querying Big Query server...\")\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client(project=GCP_PROJECT)\n",
    "    query_job = client.query(query)\n",
    "    result = query_job.result()\n",
    "    df = result.to_dataframe()\n",
    "\n",
    "    df.to_csv(data_query_cache_path, header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1) Veri Sƒ±kƒ±≈ütƒ±rma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame'imizi sayƒ±sal `dtypes` deƒüerlerini d√º≈ü√ºrerek sƒ±kƒ±≈ütƒ±ralƒ±m\n",
    "- `float64`'den `float32`'ye\n",
    "- `int64`'den `int8`'e\n",
    "\n",
    "Bunu yapmak i√ßin s√ºtunlarƒ±nda d√∂ng√º yaparƒ±z ve her biri i√ßin [`pd.to_numeric`](https://pandas.pydata.org/docs/reference/api/pandas.to_numeric.html) kullanarak `dtypes` deƒüerini m√ºmk√ºn olduƒüunca d√º≈ü√ºr√ºr√ºz\n",
    "\n",
    "**üí° 1) ML Ops Bellek Optimizasyonu `dtype` sƒ±kƒ±≈ütƒ±rma hakkƒ±nda daha fazla bilgi okuyun**\n",
    "\n",
    "**üí° 2) Daha sonra, a≈üaƒüƒ±daki kodu anlayƒ±n ve √ßalƒ±≈ütƒ±rƒ±n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(df, **kwargs):\n",
    "    \"\"\"\n",
    "    Reduces the size of the DataFrame by downcasting numerical columns\n",
    "    \"\"\"\n",
    "    input_size = df.memory_usage(index=True).sum()/ 1024**2\n",
    "    print(\"old dataframe size: \", round(input_size,2), 'MB')\n",
    "\n",
    "    in_size = df.memory_usage(index=True).sum()\n",
    "\n",
    "    for t in [\"float\", \"integer\"]:\n",
    "        l_cols = list(df.select_dtypes(include=t))\n",
    "\n",
    "        for col in l_cols:\n",
    "            df[col] = pd.to_numeric(df[col], downcast=t)\n",
    "\n",
    "    out_size = df.memory_usage(index=True).sum()\n",
    "    ratio = (1 - round(out_size / in_size, 2)) * 100\n",
    "\n",
    "    print(\"optimized size by {} %\".format(round(ratio,2)))\n",
    "    print(\"new DataFrame size: \", round(out_size / 1024**2,2), \" MB\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = compress(df, verbose=True)\n",
    "df.head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize edilmi≈ü dtypes'larƒ± kontrol edelim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° Int8 bazƒ± bilimsel k√ºt√ºphaneler i√ßin biraz √ßok k√º√ß√ºk, int16 kullanalƒ±m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.passenger_count = df.passenger_count.astype(\"int16\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° Next time, we can force dtypes directly **at loading time** as follow:\n",
    "\n",
    "```python\n",
    "query_job = client.query(query)\n",
    "result = query_job.result() \n",
    "df = result.to_dataframe(dtypes=DTYPES_RAW)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPES_RAW = {\n",
    "    \"key\": \"datetime64[ns, UTC]\",\n",
    "    \"fare_amount\": \"float32\",\n",
    "    \"pickup_datetime\": \"datetime64[ns, UTC]\",\n",
    "    \"pickup_longitude\": \"float32\",\n",
    "    \"pickup_latitude\": \"float32\",\n",
    "    \"dropoff_longitude\": \"float32\",\n",
    "    \"dropoff_latitude\": \"float32\",\n",
    "    \"passenger_count\": \"int16\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Veri Temizleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gereksiz satƒ±rlarƒ± kaldƒ±r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hatalƒ± i≈ülemleri kaldƒ±r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='any', axis=0)\n",
    "print(df.shape)\n",
    "df = df[df.passenger_count > 0]\n",
    "df = df[df.fare_amount > 0]\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coƒürafi olarak alakasƒ±z i≈ülemleri (satƒ±rlarƒ±) kaldƒ±r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check NYC bouding boxes\n",
    "# Load image of NYC map\n",
    "bounding_boxes = (-74.3, -73.7, 40.5, 40.9)\n",
    "\n",
    "url = 'https://d37p7d5kaxknzw.cloudfront/projects/nyc.png'\n",
    "nyc_map = np.array(PIL.Image.open(urllib.request.urlopen(url)))\n",
    "\n",
    "plt.imshow(nyc_map);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"pickup_latitude\"].between(left=40.5, right=40.9)]\n",
    "df = df[df[\"dropoff_latitude\"].between(left=40.5, right=40.9)]\n",
    "df = df[df[\"pickup_longitude\"].between(left=-74.3, right=-73.7)]\n",
    "df = df[df[\"dropoff_longitude\"].between(left=-74.3, right=-73.7)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's cap training set to reasonable values\n",
    "df = df[df.fare_amount < 400]\n",
    "df = df[df.passenger_count < 8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Veri G√∂rselle≈ütirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of fare\n",
    "df.fare_amount.hist(bins=100, figsize=(14,3))\n",
    "plt.xlabel('fare $USD')\n",
    "plt.title('Histogram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will be used more often to plot data on the NYC map\n",
    "def plot_on_map(df, BB, nyc_map, s=10, alpha=0.2):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16,10))\n",
    "\n",
    "    axs[0].scatter(df.pickup_longitude, df.pickup_latitude, zorder=1, alpha=alpha, c='red', s=s)\n",
    "    axs[0].set_xlim((BB[0], BB[1]))\n",
    "    axs[0].set_ylim((BB[2], BB[3]))\n",
    "    axs[0].set_title('Pickup locations')\n",
    "    axs[0].imshow(nyc_map, zorder=0, extent=BB)\n",
    "\n",
    "    axs[1].scatter(df.dropoff_longitude, df.dropoff_latitude, zorder=1, alpha=alpha, c='blue', s=s)\n",
    "    axs[1].set_xlim((BB[0], BB[1]))\n",
    "    axs[1].set_ylim((BB[2], BB[3]))\n",
    "    axs[1].set_title('Dropoff locations')\n",
    "    axs[1].imshow(nyc_map, zorder=0, extent=BB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training data on map\n",
    "plot_on_map(df, bounding_boxes, nyc_map, s=1, alpha=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_on_map(df, bounding_boxes, nyc_map, s=20, alpha=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hires(df, BB, figsize=(12, 12), ax=None, c=('r', 'b')):\n",
    "    if ax == None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    def select_within_boundingbox(df, BB):\n",
    "        return (df.pickup_longitude >= BB[0]) & (df.pickup_longitude <= BB[1]) & \\\n",
    "            (df.pickup_latitude >= BB[2]) & (df.pickup_latitude <= BB[3]) & \\\n",
    "            (df.dropoff_longitude >= BB[0]) & (df.dropoff_longitude <= BB[1]) & \\\n",
    "            (df.dropoff_latitude >= BB[2]) & (df.dropoff_latitude <= BB[3])\n",
    "\n",
    "    idx = select_within_boundingbox(df, BB)\n",
    "    ax.scatter(df[idx].pickup_longitude, df[idx].pickup_latitude, c=\"red\", s=0.01, alpha=0.5)\n",
    "    ax.scatter(df[idx].dropoff_longitude, df[idx].dropoff_latitude, c=\"blue\", s=0.01, alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hires(df, (-74.1, -73.7, 40.6, 40.9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hires(df, (-74, -73.95, 40.7, 40.8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4) Temel Skor - √ñn Sezgiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bir temel model en azƒ±ndan en belirgin √∂zelliƒüi dikkate almalƒ±dƒ±r: `pickup` ve `dropoff` arasƒ±ndaki mesafe.\n",
    "\n",
    "Doƒüru mesafe metriƒüi uygun ≈üekilde \"Manhattan mesafesi\" (L1 mesafesi) olarak adlandƒ±rƒ±lƒ±r ve iki nokta arasƒ±ndaki yatay ve dikey mesafelerin toplamƒ±nƒ± hesaplar, k√∂≈üegen (√ñklid, L2) mesafesi yerine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def manhattan_distance(start_lat: float, start_lon: float, end_lat: float, end_lon: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Manhattan distance between in km two points on the earth (specified in decimal degrees).\n",
    "    \"\"\"\n",
    "    earth_radius = 6371\n",
    "\n",
    "    lat_1_rad, lon_1_rad = math.radians(start_lat), math.radians(start_lon)\n",
    "    lat_2_rad, lon_2_rad = math.radians(end_lat), math.radians(end_lon)\n",
    "\n",
    "    dlon_rad = lon_2_rad - lon_1_rad\n",
    "    dlat_rad = lat_2_rad - lat_1_rad\n",
    "\n",
    "    manhattan_rad = abs(dlon_rad) + abs(dlat_rad)\n",
    "    manhattan_km = manhattan_rad * earth_radius\n",
    "\n",
    "    return manhattan_km\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.apply(lambda row: manhattan_distance(row[\"pickup_latitude\"], row[\"pickup_longitude\"], row[\"dropoff_latitude\"], row[\"dropoff_longitude\"]), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è A≈üaƒüƒ±daki kod biraz zaman alƒ±r ve optimize edilmemi≈ütir.\n",
    "\n",
    "Bir DataFrame √ºzerinde satƒ±r satƒ±r bir e≈üleme fonksiyonu uygulamak √ßok k√∂t√º bir m√ºhendislik pratiƒüidir, √ß√ºnk√º DataFrame'ler bellekte \"s√ºtun s√ºtun\" saklanƒ±r. \"Column-major\" veri saklama formatlarƒ± hakkƒ±nda konu≈üuyoruz. \n",
    "\n",
    "`df.apply(..., axis=1)` bir python `for` d√∂ng√ºs√ºne e≈üdeƒüerdir ve NumPy'nin vekt√∂rle≈ütirilmi≈ü i≈ülemlerini kullanmaz.\n",
    "\n",
    "üëá Bunun yerine kodumuzu vekt√∂rle≈ütirelim! Birka√ß y√ºz fakt√∂rle iyile≈ümeyi fark edin üí™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance_vectorized(df: pd.DataFrame, start_lat: str, start_lon: str, end_lat: str, end_lon: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate the Manhattan distance in km between two points on the earth (specified in decimal degrees).\n",
    "    Vectorized version for pandas df\n",
    "    \"\"\"\n",
    "    earth_radius = 6371\n",
    "\n",
    "    lat_1_rad, lon_1_rad = np.radians(df[start_lat]), np.radians(df[start_lon])\n",
    "    lat_2_rad, lon_2_rad = np.radians(df[end_lat]), np.radians(df[end_lon])\n",
    "\n",
    "    dlon_rad = lon_2_rad - lon_1_rad\n",
    "    dlat_rad = lat_2_rad - lat_1_rad\n",
    "\n",
    "    manhattan_rad = np.abs(dlon_rad) + np.abs(dlat_rad)\n",
    "    manhattan_km = manhattan_rad * earth_radius\n",
    "\n",
    "    return manhattan_km\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "manhattan_distance_vectorized(df, \"pickup_latitude\", \"pickup_longitude\",\"dropoff_latitude\", \"dropoff_longitude\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['distance'] = manhattan_distance_vectorized(df, \"pickup_latitude\", \"pickup_longitude\",\"dropoff_latitude\", \"dropoff_longitude\")\n",
    "df['distance'].hist(bins=50)\n",
    "plt.title(\"distance (km)\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manuel olarak eklediƒüimiz `distance` s√ºtununu d√º≈ü√ºrelim ve ≈üimdi ger√ßek bir √∂n i≈üleme pipeline'ƒ± olu≈üturalƒ±m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['distance'])\n",
    "df.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Eƒüitim/Doƒürulama/Test Ayrƒ±mƒ±"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üö® Zaman damgalƒ± verilerle uƒüra≈üƒ±yoruz:\n",
    "- Eƒüitim/doƒürulama/test ayrƒ±mƒ±nƒ± **kronolojik** bir ≈üekilde yapmak √∂nemlidir.\n",
    "- √áok uzun doƒürulama veya test k√ºmelerini tutmak istemeyiz: ger√ßek hayatta makroekonomik ko≈üullar hƒ±zla deƒüi≈üiyor!\n",
    "- `split_ratio` ne kadar k√º√ß√ºk olursa, modelimizi o kadar sƒ±k yeniden eƒüitmemiz gerekecek\n",
    "- 6 yƒ±llƒ±k bir veri k√ºmesiyle, doƒürulama i√ßin 1 ay ilerisi, test i√ßin 1 ay ilerisi tutmak tamamen uygun\n",
    "- √úretimde, bu model performansƒ±mƒ±zƒ±n gelecekte 1 aydan fazla uzanmasƒ±na g√ºvenmememiz gerektiƒüi anlamƒ±na gelir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.02 # ~1 month for val, ~1 month for test\n",
    "\n",
    "test_length = int(len(df) * split_ratio)\n",
    "val_length = int((len(df)-test_length) * split_ratio)\n",
    "train_length = len(df) - val_length - test_length\n",
    "\n",
    "df_train = df.iloc[:train_length, :].sample(frac=1) # Shuffle datasets to improve training\n",
    "df_val = df.iloc[train_length: train_length + val_length, :].sample(frac=1)\n",
    "df_test = df.iloc[train_length+val_length:, :].sample(frac=1)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "assert len(df_train) + len(df_val) + len(df_test) == len(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.pickup_datetime.min())\n",
    "print(df_train.pickup_datetime.max())\n",
    "print('---')\n",
    "print(df_val.pickup_datetime.min())\n",
    "print(df_val.pickup_datetime.max())\n",
    "print('---')\n",
    "print(df_test.pickup_datetime.min())\n",
    "print(df_test.pickup_datetime.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop(\"fare_amount\", axis=1)\n",
    "y = df[[\"fare_amount\"]]\n",
    "\n",
    "X_train = df_train.drop(\"fare_amount\", axis=1)\n",
    "y_train = df_train[[\"fare_amount\"]]\n",
    "\n",
    "X_val = df_val.drop(\"fare_amount\", axis=1)\n",
    "y_val = df_val[[\"fare_amount\"]]\n",
    "\n",
    "X_test = df_test.drop(\"fare_amount\", axis=1)\n",
    "y_test = df_test[[\"fare_amount\"]]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basit Temel: $price = a * distance + b $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_train = np.array(manhattan_distance_vectorized(X_train, \"pickup_latitude\", \"pickup_longitude\",\"dropoff_latitude\", \"dropoff_longitude\"))\n",
    "distances_val = np.array(manhattan_distance_vectorized(X_val, \"pickup_latitude\", \"pickup_longitude\",\"dropoff_latitude\", \"dropoff_longitude\"))\n",
    "distances_test = np.array(manhattan_distance_vectorized(X_test, \"pickup_latitude\", \"pickup_longitude\",\"dropoff_latitude\", \"dropoff_longitude\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=distances_train, y=y_train);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "pearson, p_value = pearsonr(distances_train, y_train)\n",
    "print(f'{pearson=}')\n",
    "print(f'{p_value=}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "baseline_model = LinearRegression()\n",
    "baseline_model.fit(distances_train[:, None], y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_pred_val = baseline_model.predict(distances_val[:, None])\n",
    "baseline_pred_test = baseline_model.predict(distances_test[:, None])\n",
    "baseline_mae_val = np.mean(np.abs(baseline_pred_val - y_val), axis=0)\n",
    "baseline_mae_test = np.mean(np.abs(baseline_pred_test - y_test), axis=0)\n",
    "\n",
    "print(f'mean taxifare prices on train set = {round(float(np.mean(y_train, axis=0)),2)} $')\n",
    "print(f'üéØ baseline MAE on val set = {round(float(baseline_mae_val),2)} $')\n",
    "print(f'üéØ baseline MAE on test set = {round(float(baseline_mae_test),2)} $')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) √ñn ƒ∞≈üleme Pipeline'ƒ±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bize yalnƒ±zca 5 √∂zellik (yolcular + boylam/enlem) ve potansiyel olarak on milyonlarca satƒ±r i√ßeren bir veri k√ºmesi veriliyor.\n",
    "\n",
    "üëâ \"g√ºn√ºn saati\" gibi bir s√ºr√º \"m√ºhendislik\" √∂zelliƒüi olu≈üturmak tamamen mantƒ±klƒ±  \n",
    "- Y√ºzlercesi hi√ßbir problem yaratmaz √ß√ºnk√º b√ºy√ºk sayƒ±da satƒ±r modelimizin bu √ßoklu √∂zelliklerle ili≈ükili t√ºm aƒüƒ±rlƒ±klarƒ± √∂ƒürenmesini saƒülayacaktƒ±r\n",
    "- Yoƒüun, Derin √ñƒürenme aƒüƒ± b√∂yle bir durum i√ßin √ßok uygundur\n",
    "\n",
    "‚ùóÔ∏è A≈üaƒüƒ±da √∂nerilen √∂n i≈ülemci **sabit sayƒ±da √∂zellik** (65) √ßƒ±karƒ±r ve bu **eƒüitim k√ºmesinden baƒüƒ±msƒ±zdƒ±r**. Bunu y√ºz milyonlarca satƒ±ra √∂l√ßeklendirirken i≈üe yarayacaƒüƒ±nƒ± g√∂receksiniz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Yolcu √ñn ƒ∞≈ülemcileri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yolcu sayƒ±larƒ±nƒ± analiz edelim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.passenger_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x=\"passenger_count\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASSENGER PIPE\n",
    "p_min = 1.\n",
    "p_max = 8.\n",
    "passenger_pipe = FunctionTransformer(lambda p: (p-p_min)/(p_max-p_min))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"passenger_scaler\", passenger_pipe, [\"passenger_count\"]),\n",
    "    ],\n",
    ")\n",
    "preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(preprocessor.fit_transform(X_train))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) Zaman √ñn ƒ∞≈ülemcisi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pickup_datetime`'dan ilgin√ß √∂znitelikleri √ßƒ±karalƒ±m\n",
    "- g√ºn√ºn saati\n",
    "- haftanƒ±n g√ºn√º\n",
    "- yƒ±lƒ±n ayƒ±\n",
    "- 2009'dan beri g√ºn sayƒ±sƒ± (enflasyon parametrelerini kodlayabilir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def transform_time_features(X: pd.DataFrame) -> np.ndarray:\n",
    "    assert isinstance(X, pd.DataFrame)\n",
    "\n",
    "    timedelta = (X[\"pickup_datetime\"] - pd.Timestamp('2009-01-01T00:00:00', tz='UTC'))/pd.Timedelta(1,'D')\n",
    "\n",
    "    pickup_dt = X[\"pickup_datetime\"].dt.tz_convert(\"America/New_York\").dt\n",
    "    dow = pickup_dt.weekday\n",
    "    hour = pickup_dt.hour\n",
    "    month = pickup_dt.month\n",
    "\n",
    "    hour_sin = np.sin(2 * math.pi / 24 * hour)\n",
    "    hour_cos = np.cos(2*math.pi / 24 * hour)\n",
    "\n",
    "    return np.stack([hour_sin, hour_cos, dow, month, timedelta], axis=1)\n",
    "\n",
    "X_time_processed = transform_time_features(X[[\"pickup_datetime\"]])\n",
    "\n",
    "pd.DataFrame(X_time_processed, columns=[\"hour_sin\", \"hour_cos\", \"dow\", \"month\", \"timedelta\"]).head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daha sonra, t√ºm 24*7 kategori kombinasyonlarƒ±nƒ± `X_processed`'da her zaman mevcut olmaya zorlayarak `[\"haftanƒ±n g√ºn√º\", \"ay\"]`'ƒ± one-hot kodlarƒ±z \n",
    "\n",
    "(sonunda `X_processed` i√ßin sabit bir boyut istediƒüimizi unutmayƒ±n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_categories = [\n",
    "        np.arange(0, 7, 1),  # days of the week from 0 to 6\n",
    "        np.arange(1, 13, 1)  # months of the year from 1 to 12\n",
    "    ]\n",
    "\n",
    "OneHotEncoder(categories=time_categories, sparse_output=False)\\\n",
    "    .fit_transform(X_time_processed[:,[2,3]]) # column index [2,3] for ['dow', 'month'] !\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ve bunu `timedelta` s√ºtununun bir t√ºr \"Min-Max\" yeniden √∂l√ßeklemesi ile birle≈ütirin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_time_processed[:,-1].min())\n",
    "print(X_time_processed[:,-1].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedelta_min = 0\n",
    "timedelta_max = 2190 # Our model may extend in the future. No big deal if the scaled data extend slightly beyond 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_pipe = make_pipeline(\n",
    "    FunctionTransformer(transform_time_features),\n",
    "    make_column_transformer(\n",
    "        (OneHotEncoder(\n",
    "            categories=time_categories,\n",
    "            sparse_output=False,\n",
    "            handle_unknown=\"ignore\"\n",
    "        ), [2, 3]), # corresponds to columns [\"day of week\", \"month\"], not the other columns\n",
    "\n",
    "        (FunctionTransformer(lambda year: (year - timedelta_min) / (timedelta_max - timedelta_min)), [4]), # min-max scale the columns 4 [\"year\"]\n",
    "        remainder=\"passthrough\" # keep hour_sin and hour_cos\n",
    "    )\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"passenger_scaler\", passenger_pipe, [\"passenger_count\"]),\n",
    "        (\"time_preproc\", time_pipe, [\"pickup_datetime\"]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(preprocessor.fit_transform(X_train)).plot(kind='box');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Yakla≈üƒ±k olarak ortalanmƒ±≈ü ve √∂l√ßeklenmi≈ü 23 √∂zellik"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3) Mesafe Pipeline'ƒ±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hem haversine hem de Manhattan mesafelerini √∂zellik olarak ekleyelim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonlat_features = [\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances_vectorized(df: pd.DataFrame, start_lat: str, start_lon: str, end_lat: str, end_lon: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate the haversine and Manhattan distances between two points (specified in decimal degrees).\n",
    "    Vectorized version for pandas df\n",
    "    Computes distance in Km\n",
    "    \"\"\"\n",
    "    earth_radius = 6371\n",
    "\n",
    "    lat_1_rad, lon_1_rad = np.radians(df[start_lat]), np.radians(df[start_lon])\n",
    "    lat_2_rad, lon_2_rad = np.radians(df[end_lat]), np.radians(df[end_lon])\n",
    "\n",
    "    dlon_rad = lon_2_rad - lon_1_rad\n",
    "    dlat_rad = lat_2_rad - lat_1_rad\n",
    "\n",
    "    manhattan_rad = np.abs(dlon_rad) + np.abs(dlat_rad)\n",
    "    manhattan_km = manhattan_rad * earth_radius\n",
    "\n",
    "    a = (np.sin(dlat_rad / 2.0)**2 + np.cos(lat_1_rad) * np.cos(lat_2_rad) * np.sin(dlon_rad / 2.0)**2)\n",
    "    haversine_rad = 2 * np.arcsin(np.sqrt(a))\n",
    "    haversine_km = haversine_rad * earth_radius\n",
    "\n",
    "    return dict(\n",
    "        haversine = haversine_km,\n",
    "        manhattan = manhattan_km\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_lonlat_features(X:pd.DataFrame)-> pd.DataFrame:\n",
    "    assert isinstance(X, pd.DataFrame)\n",
    "    res = distances_vectorized(X, *lonlat_features)\n",
    "\n",
    "    return pd.DataFrame(res)\n",
    "\n",
    "distances = transform_lonlat_features(X[lonlat_features])\n",
    "distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_min = 0\n",
    "dist_max = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_pipe = make_pipeline(\n",
    "    FunctionTransformer(transform_lonlat_features),\n",
    "    FunctionTransformer(lambda dist: (dist - dist_min) / (dist_max - dist_min))\n",
    "    )\n",
    "distance_pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"passenger_scaler\", passenger_pipe, [\"passenger_count\"]),\n",
    "        (\"time_preproc\", time_pipe, [\"pickup_datetime\"]),\n",
    "        (\"dist_preproc\", distance_pipe, lonlat_features),\n",
    "    ],\n",
    ")\n",
    "preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed = pd.DataFrame(preprocessor.fit_transform(X_train))\n",
    "X_processed.plot(kind='box');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Yakla≈üƒ±k olarak √∂l√ßeklenmi≈ü 25 √∂zellik"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4) GeoHasher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son olarak, **b√∂lgeler** hakkƒ±nda bilgi ekleyelim! \n",
    "\n",
    "Bazƒ±larƒ± diƒüerlerinden daha pahalƒ± olabilir (√∂rn. JFK havalimanƒ±!)\n",
    "\n",
    "Jeouzaysal bilgiyi _bucketize_ etmek i√ßin `pygeohash` kullanacaƒüƒ±z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygeohash as gh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° pygeohash (lat,lon)'u se√ßilen kesinliklerde jeospacial \"kare kovalar\"a d√∂n√º≈üt√ºr√ºr. Ne kadar √ßok kesinlik isterseniz, o kadar √ßok \"kova\" olasƒ±lƒ±ƒüƒ± vardƒ±r!\n",
    "\n",
    "<img src=\"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/07-ML-OPS/geohashes.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = X_train.iloc[0,:]\n",
    "(x0.pickup_latitude, x0.pickup_longitude)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gh.encode(x0.pickup_latitude, x0.pickup_longitude, precision=3))\n",
    "print(gh.encode(x0.pickup_latitude, x0.pickup_longitude, precision=4))\n",
    "print(gh.encode(x0.pickup_latitude, x0.pickup_longitude, precision=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Bunu T√úM veri setumuze uygulayalƒ±m (bu √∂n i≈ülemenin √ßok uzun zaman alabileceƒüini unutmayƒ±n!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_geohash(X:pd.DataFrame, precision:int = 5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Add a geohash (ex: \"dr5rx\") of len \"precision\" = 5 by default\n",
    "    corresponding to each (lon, lat) tuple, for pick-up and drop-off\n",
    "    \"\"\"\n",
    "    assert isinstance(X, pd.DataFrame)\n",
    "\n",
    "    X[\"geohash_pickup\"] = X.apply(\n",
    "        lambda x: gh.encode(x.pickup_latitude, x.pickup_longitude, precision=precision),\n",
    "        axis=1\n",
    "    )\n",
    "    X[\"geohash_dropoff\"] = X.apply(\n",
    "        lambda x: gh.encode(x.dropoff_latitude, x.dropoff_longitude, precision=precision),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return X[[\"geohash_pickup\", \"geohash_dropoff\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_geohash(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Bu sefer `pygeohash`'i `df.apply(axis=1)` ile satƒ±r satƒ±r uygulamaktan ba≈üka se√ßeneƒüimiz olmadƒ±ƒüƒ±nƒ± ve hesaplamanƒ±n biraz zaman aldƒ±ƒüƒ±nƒ± fark edin.\n",
    "\n",
    "Bu, her zaman vekt√∂rle≈ütirilmemi≈ü harici bir Python k√ºt√ºphanesine g√ºvenmenin tehlikesidir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá En yaygƒ±n b√∂lgeler hangileri?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_geohashes = pd.concat([\n",
    "    X_train.apply(lambda x: gh.encode(x.pickup_latitude, x.pickup_longitude, precision=4), axis=1),\n",
    "    X_train.apply(lambda x: gh.encode(x.dropoff_latitude, x.dropoff_longitude, precision=4), axis=1),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_geohashes.value_counts()))\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(np.cumsum(all_geohashes.value_counts()[:20])/(2*len(X_train))*100)\n",
    "plt.title(\"percentage of taxi rides from/to these districts\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Sadece ilk 20 b√∂lge √∂nemli. Bunlarƒ± one-hot kodlayabiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_important_geohash_districts = np.array(all_geohashes.value_counts()[:20].index)\n",
    "most_important_geohash_districts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's hard-code below the 20 most frequent district GeoHashes of precision 5,\n",
    "# covering about 99% of all dropoff/pickup locations.\n",
    "most_important_geohash_districts = [\n",
    "    \"dr5ru\", \"dr5rs\", \"dr5rv\", \"dr72h\", \"dr72j\", \"dr5re\", \"dr5rk\",\n",
    "    \"dr5rz\", \"dr5ry\", \"dr5rt\", \"dr5rg\", \"dr5x1\", \"dr5x0\", \"dr72m\",\n",
    "    \"dr5rm\", \"dr5rx\", \"dr5x2\", \"dr5rw\", \"dr5rh\", \"dr5x8\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her GeoHash'i yukarƒ±da listelenen ilk 20 farklƒ± kovadan birinde one-hot kodlayalƒ±m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geohash_categories = [\n",
    "    most_important_geohash_districts,  # pickup district list\n",
    "    most_important_geohash_districts  # dropoff district list\n",
    "]\n",
    "\n",
    "geohash_pipe = make_pipeline(\n",
    "    FunctionTransformer(compute_geohash),\n",
    "    OneHotEncoder(\n",
    "        categories=geohash_categories,\n",
    "        handle_unknown=\"ignore\",\n",
    "        sparse_output=False\n",
    "    )\n",
    ")\n",
    "geohash_pipe\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5) Tam √ñn ƒ∞≈üleme Pipeline'ƒ±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son √∂n i≈ülemcimizi √∂zetleyelim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Kodlayƒ±cƒ±lar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygeohash as gh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_time_features(X: pd.DataFrame) -> np.ndarray:\n",
    "    assert isinstance(X, pd.DataFrame)\n",
    "\n",
    "    timedelta = (X[\"pickup_datetime\"] - pd.Timestamp('2009-01-01T00:00:00', tz='UTC'))/pd.Timedelta(1,'D')\n",
    "\n",
    "    pickup_dt = X[\"pickup_datetime\"].dt.tz_convert(\"America/New_York\").dt\n",
    "    dow = pickup_dt.weekday\n",
    "    hour = pickup_dt.hour\n",
    "    month = pickup_dt.month\n",
    "\n",
    "    hour_sin = np.sin(2 * math.pi / 24 * hour)\n",
    "    hour_cos = np.cos(2*math.pi / 24 * hour)\n",
    "\n",
    "    return np.stack([hour_sin, hour_cos, dow, month, timedelta], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_lonlat_features(X: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    assert isinstance(X, pd.DataFrame)\n",
    "    lonlat_features = [\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"]\n",
    "\n",
    "    def distances_vectorized(df: pd.DataFrame, start_lat: str, start_lon: str, end_lat: str, end_lon: str) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate the haversine and Manhattan distances between two points on the earth (specified in decimal degrees).\n",
    "        Vectorized version for pandas df\n",
    "        Computes distance in Km\n",
    "        \"\"\"\n",
    "        earth_radius = 6371\n",
    "\n",
    "        lat_1_rad, lon_1_rad = np.radians(df[start_lat]), np.radians(df[start_lon])\n",
    "        lat_2_rad, lon_2_rad = np.radians(df[end_lat]), np.radians(df[end_lon])\n",
    "\n",
    "        dlon_rad = lon_2_rad - lon_1_rad\n",
    "        dlat_rad = lat_2_rad - lat_1_rad\n",
    "\n",
    "        manhattan_rad = np.abs(dlon_rad) + np.abs(dlat_rad)\n",
    "        manhattan_km = manhattan_rad * earth_radius\n",
    "\n",
    "        a = (np.sin(dlat_rad / 2.0)**2 + np.cos(lat_1_rad) * np.cos(lat_2_rad) * np.sin(dlon_rad / 2.0)**2)\n",
    "        haversine_rad = 2 * np.arcsin(np.sqrt(a))\n",
    "        haversine_km = haversine_rad * earth_radius\n",
    "\n",
    "        return dict(\n",
    "            haversine=haversine_km,\n",
    "            manhattan=manhattan_km)\n",
    "\n",
    "    result = pd.DataFrame(distances_vectorized(X, *lonlat_features))\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_geohash(X: pd.DataFrame, precision: int = 5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Add a GeoHash (ex: \"dr5rx\") of len \"precision\" = 5 by default\n",
    "    corresponding to each (lon, lat) tuple, for pick-up, and drop-off\n",
    "    \"\"\"\n",
    "    assert isinstance(X, pd.DataFrame)\n",
    "\n",
    "    X[\"geohash_pickup\"] = X.apply(\n",
    "        lambda x: gh.encode(x.pickup_latitude, x.pickup_longitude, precision=precision),\n",
    "        axis=1\n",
    "    )\n",
    "    X[\"geohash_dropoff\"] = X.apply(\n",
    "        lambda x: gh.encode(x.dropoff_latitude, x.dropoff_longitude, precision=precision),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return X[[\"geohash_pickup\", \"geohash_dropoff\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASSENGER PIPE\n",
    "p_min = 1\n",
    "p_max = 8\n",
    "passenger_pipe = FunctionTransformer(lambda p: (p - p_min) / (p_max - p_min))\n",
    "\n",
    "# DISTANCE PIPE\n",
    "dist_min = 0\n",
    "dist_max = 100\n",
    "\n",
    "distance_pipe = make_pipeline(\n",
    "    FunctionTransformer(transform_lonlat_features),\n",
    "    FunctionTransformer(lambda dist: (dist - dist_min) / (dist_max - dist_min))\n",
    ")\n",
    "\n",
    "# TIME PIPE\n",
    "timedelta_min = 0\n",
    "timedelta_max = 2090\n",
    "\n",
    "time_categories = [\n",
    "    np.arange(0, 7, 1),  # days of the week\n",
    "    np.arange(1, 13, 1)  # months of the year\n",
    "]\n",
    "\n",
    "time_pipe = make_pipeline(\n",
    "    FunctionTransformer(transform_time_features),\n",
    "    make_column_transformer(\n",
    "        (OneHotEncoder(\n",
    "            categories=time_categories,\n",
    "            sparse_output=False,\n",
    "            handle_unknown=\"ignore\"\n",
    "        ), [2,3]), # corresponds to columns [\"day of week\", \"month\"], not the other columns\n",
    "\n",
    "        (FunctionTransformer(lambda year: (year - timedelta_min) / (timedelta_max - timedelta_min)), [4]), # min-max scale the columns 4 [\"timedelta\"]\n",
    "        remainder=\"passthrough\" # keep hour_sin and hour_cos\n",
    "    )\n",
    ")\n",
    "\n",
    "# GEOHASH PIPE\n",
    "lonlat_features = [\n",
    "    \"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\",\n",
    "    \"dropoff_longitude\"\n",
    "]\n",
    "\n",
    "# Below are the 20 most frequent district geohashes of precision 5,\n",
    "# covering about 99% of all dropoff/pickup locations,\n",
    "# according to prior analysis in a separate notebook\n",
    "most_important_geohash_districts = [\n",
    "    \"dr5ru\", \"dr5rs\", \"dr5rv\", \"dr72h\", \"dr72j\", \"dr5re\", \"dr5rk\",\n",
    "    \"dr5rz\", \"dr5ry\", \"dr5rt\", \"dr5rg\", \"dr5x1\", \"dr5x0\", \"dr72m\",\n",
    "    \"dr5rm\", \"dr5rx\", \"dr5x2\", \"dr5rw\", \"dr5rh\", \"dr5x8\"\n",
    "]\n",
    "\n",
    "geohash_categories = [\n",
    "    most_important_geohash_districts,  # pickup district list\n",
    "    most_important_geohash_districts  # dropoff district list\n",
    "]\n",
    "\n",
    "geohash_pipe = make_pipeline(\n",
    "    FunctionTransformer(compute_geohash),\n",
    "    OneHotEncoder(\n",
    "        categories=geohash_categories,\n",
    "        handle_unknown=\"ignore\",\n",
    "        sparse_output=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# COMBINED PREPROCESSOR\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"passenger_scaler\", passenger_pipe, [\"passenger_count\"]),\n",
    "        (\"time_preproc\", time_pipe, [\"pickup_datetime\"]),\n",
    "        (\"dist_preproc\", distance_pipe, lonlat_features),\n",
    "        (\"geohash\", geohash_pipe, lonlat_features),\n",
    "    ],\n",
    "    n_jobs=-1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preprocessor.fit(X_train)\n",
    "\n",
    "X_train_processed = final_preprocessor.transform(X_train)\n",
    "X_val_processed = final_preprocessor.transform(X_val)\n",
    "X_test_processed = final_preprocessor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "pd.DataFrame(X_train_processed).plot(kind='box', ax=ax);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "sns.heatmap(pd.DataFrame(X_train_processed).corr(), vmin=-1, cmap='RdBu');\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sonu√ß olarak, √ßok fazla performans kaybetmeden i≈ülenmi≈ü verimizi muhtemelen float32'ye de sƒ±kƒ±≈ütƒ±rabiliriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_processed.nbytes / 1024**2, \"MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress the data a bit\n",
    "X_train_processed = X_train_processed.astype(np.float32)\n",
    "X_val_processed = X_val_processed.astype(np.float32)\n",
    "X_test_processed = X_test_processed.astype(np.float32)\n",
    "\n",
    "print(X_train_processed.nbytes / 1024**2, \"MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_processed).describe().mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è √ñn i≈ülemci eƒüitim k√ºmesinden baƒüƒ±msƒ±z **sabit** sayƒ±da √∂zellik (65) √ßƒ±karƒ±r. \n",
    "\n",
    "‚òùÔ∏è √ñn i≈ülemci aynƒ± zamanda **durumsuz**dur (yani `.fit()` metodu yoktur, yalnƒ±zca `.transform()` vardƒ±r). Bu, standart √∂l√ßeklemenin aksine, \"X_train standart sapmalarƒ±\"nƒ± i√ß durumlar olarak saklamak zorunda olan saf bir fonksiyon $f:X \\rightarrow X_{processed}$ olarak g√∂r√ºlebilir.\n",
    "\n",
    "Bu iki √∂zellik, ML M√ºhendisliƒüi ekibinin √∂n i≈ülemeyi y√ºzlerce GB'lara √∂l√ßeklendirmek i√ßin i≈ülerini √ßok daha kolay hale getirecektir."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) Mimari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import Model, Sequential, layers, regularizers\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(input_shape:tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Initialize the Neural Network with random weights\n",
    "    \"\"\"\n",
    "\n",
    "    reg = regularizers.l1_l2(l1=0.005)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Input(shape=input_shape))\n",
    "    model.add(layers.Dense(100, activation=\"relu\", kernel_regularizer=reg))\n",
    "    model.add(layers.BatchNormalization(momentum=0.9))\n",
    "    model.add(layers.Dropout(rate=0.1))\n",
    "    model.add(layers.Dense(50, activation=\"relu\"))\n",
    "    model.add(layers.BatchNormalization(momentum=0.9))  # use momentum=0 to only use statistic of the last seen minibatch in inference mode (\"short memory\"). Use 1 to average statistics of all seen batch during training histories.\n",
    "    model.add(layers.Dropout(rate=0.1))\n",
    "    model.add(layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "    print(\"‚úÖ model initialized\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model(input_shape=X_train_processed.shape[1:])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "batch_size = 256\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"mae\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=2,\n",
    "    restore_best_weights=True,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_processed,\n",
    "    y_train,\n",
    "    validation_data=(X_val_processed, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) Performans deƒüerlendirmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE val\", round(model.evaluate(X_val_processed, y_val)[1],2), ' $')\n",
    "print(\"MAE test\", round(model.evaluate(X_test_processed, y_test)[1],2), ' $')\n",
    "print(\"MAE test baseline\", round(float(baseline_mae_test),2), ' $')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "plt.hist(y_pred, label='pred', color='r', bins=200, alpha=0.3)\n",
    "plt.hist(y_test, label='truth', color='b', bins=200, alpha=0.3)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim((0,60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_pred - y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_pred - y_test\n",
    "sns.histplot(residuals)\n",
    "plt.xlim(xmin=-20, xmax=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals.sort_values(by='fare_amount')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual vs. actual scatter plot\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.scatter(x=y_test,y=residuals, alpha=0.1)\n",
    "plt.xlabel('actual')\n",
    "plt.ylabel('residuals')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual vs. predicted scatter plot\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.scatter(x=y_pred,y=residuals, alpha=0.1)\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('residuals')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Sƒ±nƒ±rlƒ± 60k veri seti √ºzerinde eƒüitilen modelimizin ortalama yolculuk fiyatƒ± 11$'a kƒ±yasla yolculuk ba≈üƒ±na yakla≈üƒ±k 2.5$ MAE'si var.  \n",
    "\n",
    "Basit bir doƒürusal regresyon bize yakla≈üƒ±k 3$ MAE verir, ama ≈üeytan ayrƒ±ntƒ±larda gizlidir!\n",
    "\n",
    "√ñzellikle, √ßok uzun/pahalƒ± yolculuklarƒ± tahmin etmede o kadar iyi deƒüiliz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Anlayƒ±≈üƒ±nƒ±zƒ± Test Edin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Arkada≈üƒ±nƒ±zla bu sorularƒ± cevaplamaya √ßalƒ±≈üƒ±n\n",
    "1. Modelin performansƒ±ndan memnun musunuz? \n",
    "2. Geli≈ütirmek i√ßin herhangi bir fikirler?\n",
    "3. Durumsuz pipeline nedir (durumlu olanƒ±n aksine)?\n",
    "4. OHEncoder sabit s√ºtun kategorileri ile nasƒ±l √ßalƒ±≈üƒ±r?\n",
    "5. Sinir Aƒüƒ±nda veri normalle≈ütirme nasƒ±l yapƒ±lƒ±r?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary markdown='span'>üí° Cevaplar</summary>\n",
    "\n",
    "1. Doƒürusal regresyon temeline g√∂re %25 iyile≈ümemiz var (ama emin olmak i√ßin bunu √ßapraz doƒürulamamƒ±z gerekir). Ayrƒ±ca, fiyatlarƒ± trafiƒüe de baƒülƒ± olan taksi yolculuklarƒ±nda 2$'lƒ±k tahmin hatasƒ± indirgenemez hata oranƒ±na yakƒ±n g√∂r√ºn√ºyor.\n",
    "\n",
    "\n",
    "2. Daha fazla eƒüitim verisi ekleyerek modeli geli≈ütirebiliriz (bunu daha sonra test edeceƒüiz). Ba≈üka bir umut verici fikir kategorik √∂zelliklerimizin g√∂m√ºlmesini *√∂ƒürenmek* olabilir (onlarƒ± one-hot kodlamak yerine).\n",
    "\n",
    "\n",
    "3. Durumsuz pipeline ger√ßek bir `.fit()` metoduna sahip deƒüildir, yalnƒ±zca `.transform()` vardƒ±r. \n",
    "\n",
    "\n",
    "4. Durumsuz olmak i√ßin, one-hot kodlanacak `kategorileri` sabit kodladƒ±k `OneHotEncoder(categories=categories,...)` ve √∂l√ßekleyicilerimizde her s√ºtunun istatistiksel √∂zelliklerini sabit kodladƒ±k:  `FunctionTransformer(lambda dist: (dist - dist_min)/(dist_max - dist_min))`\n",
    "\n",
    "\n",
    "5. TensorFlow modelinde, her yoƒüun katman arasƒ±na eklediƒüimiz `layers.BatchNormalization()`'ƒ± fark edin, bu da verileri batch-per-batch normalle≈ütirir! Geri yayƒ±lƒ±m sƒ±rasƒ±nda gradyan kaybolmasƒ±nƒ± d√ºzeltmeye yardƒ±mcƒ± olan harika bir √∂zelliktir!\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì A≈üaƒüƒ±daki bu yeni yolculuk `X_new` i√ßin fiyat tahmin edin ve sonucu `y_new` olarak `np.ndarray` ≈üeklinde saklayƒ±n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.iloc[0:2,:]['pickup_datetime']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pd.DataFrame(dict(\n",
    "    pickup_datetime=[pd.Timestamp(\"2013-07-06 17:18:00\", tz='UTC')],\n",
    "    pickup_longitude=[-73.950655],\n",
    "    pickup_latitude=[40.783282],\n",
    "    dropoff_longitude=[-73.984365],\n",
    "    dropoff_latitude=[40.769802],\n",
    "    passenger_count=[1],\n",
    "))\n",
    "\n",
    "X_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute y_new\n",
    "pass  # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult(\n",
    "    'notebook',\n",
    "    subdir='train_at_scale',\n",
    "    y_new=y_new\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son olarak, Kitt'in ilerlemenizi takip edebilmesi i√ßin `make test_kitt` √ßalƒ±≈ütƒ±ralƒ±m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd .. && make test_kitt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxifare-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
